{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "datapath1 = 'F:/UChicago/degree paper/Archive/'\n",
    "datapath2 = 'F:/UChicago/degree paper/glove.6B/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "new_data=[]\n",
    "text=[]\n",
    "star=[]\n",
    "## Save a new data\n",
    "with open(\"F:/UChicago/degree paper/yelp_dataset/review.json\",'r', encoding='UTF-8') as load_f:\n",
    "    for jsonstr in load_f.readlines():\n",
    "        if i>=1000:\n",
    "            break\n",
    "        # 将josn字符串转化为dict字典\n",
    "        jsonstr = json.loads(jsonstr)\n",
    "        new_data.append(np.array((jsonstr['text'],int(jsonstr['stars'])),dtype=object))\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(datapath1 + 'yelp_review_1000.npz',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataname = 'yelp_review.npz'\n",
    "npz = np.load(datapath1 + dataname, allow_pickle=True)\n",
    "data = npz['arr_0']     # Loaded medium size dataset\n",
    "X = data[:,0]\n",
    "Y = data[:,1]\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like walking back in time, every Saturday morning my sister and I was in a bowling league and after we were done, we\\'d spend a few quarters playing the pin ball machines until our mother came to pick us up.\\n\\nMy sister was daring and play the machines hard, she was afraid of that \"tilt\" showing up and freezing the game.  I, on the other hand was a bit more gentler and wanted to make sure I got my quarter\\'s worth.\\n\\nThis place has rows and rows of machines, some are really old and some are more of a mid 80\\'s theme.  There is even a Ms pac man!  It was fun to spend an afternoon playing the machines and remembering all the fun of my early teen years.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation+\"\\n\")) # Remove \\n for document embedding\n",
    "\n",
    "def ConvertSentence2Word(s):\n",
    "    return(word_tokenize(remove_punctuation(s).lower()))\n",
    "\n",
    "def Preprocess(EMBEDDING_DIM=50):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    bagofwords=Counter()\n",
    "    num_docs = len(X)\n",
    "\n",
    "    for i in range(num_docs):\n",
    "        words = [w for w in ConvertSentence2Word(X[i]) if not w in stop]\n",
    "        for w in words:\n",
    "            bagofwords[w] += 1\n",
    "\n",
    "    bagofwords={k:v for k,v in bagofwords.items() if v >= 3} # Remove items apper less than 3\n",
    "    # Word to Index\n",
    "    word2idx = {w:i+1 for i, w in enumerate(bagofwords)} # Start from 1\n",
    "    num_words = len(word2idx)    \n",
    "\n",
    "    # load in pre-trained word vectors\n",
    "    word2vec = {}\n",
    "    start = time.time()\n",
    "    with open(os.path.join(datapath2+'glove.6B.%sd.txt' % EMBEDDING_DIM),encoding='UTF8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM)) # Index 0 will be zero\n",
    "    for word, i in word2idx.items():\n",
    "        if word in word2vec:\n",
    "            embedding_matrix[i] = word2vec[word]\n",
    "    \n",
    "    return word2idx, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word2idx, embedding_matrix = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# save the model\n",
    "with open(datapath1 + 'word2idx_yelp_1000.json', 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "    \n",
    "np.savez(datapath1 + 'WordEmbedding_yelp_1000.npz', embedding_matrix )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "datapath1 = 'F:/UChicago/degree paper/Archive/'\n",
    "datapath2 = 'F:/UChicago/degree paper/glove.6B/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "new_data=[]\n",
    "text=[]\n",
    "star=[]\n",
    "## Save a new data\n",
    "with open(\"F:/UChicago/degree paper/yelp_dataset/Pet_Supplies_5.json\",'r', encoding='UTF-8') as load_f:\n",
    "    for jsonstr in load_f.readlines():\n",
    "        if i>=(100000):\n",
    "            break\n",
    "        # 将josn字符串转化为dict字典\n",
    "        jsonstr = json.loads(jsonstr)\n",
    "        new_data.append(np.array((jsonstr['reviewText'],int(jsonstr['overall'])),dtype=object))\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(datapath1 + 'amazon_review_100k_pet.npz',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'amazon_review_100k_pet.npz'\n",
    "npz = np.load(datapath1 + dataname, allow_pickle=True)\n",
    "data = npz['arr_0']     # Loaded medium size dataset\n",
    "X = data[:,0]\n",
    "Y = data[:,1]\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My dogs have a variety of Nylabones and one from Hartz as well and they all get chewed on depending on what my boys are in the mood for.  They regularly compete to be the one to get dinosaur, so I would rank that as this one being a favorite.  Both my dogs are heavy chewers, but the nylabones seem to last a good long while and they never get tired of them.  One other thing, the dinosaur (with all the little nubs) is probably the most horrible thing to step on when making a trip to the bathroom in the middle of the night!'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 5755, 2: 5726, 3: 9850, 4: 17059, 5: 61610})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counter(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataname = 'yelp_review.npz'\n",
    "#npz = np.load(datapath1 + dataname, allow_pickle=True)\n",
    "#data = npz['arr_0']     # Loaded medium size dataset\n",
    "#X1 = data[:,0]\n",
    "#Y1 = data[:,1]\n",
    "#del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation+\"\\n\")) # Remove \\n for document embedding\n",
    "\n",
    "def ConvertSentence2Word(s):\n",
    "    return(word_tokenize(remove_punctuation(s).lower()))\n",
    "\n",
    "def Preprocess(EMBEDDING_DIM=50):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    bagofwords=Counter()\n",
    "    num_docs = len(X)\n",
    "\n",
    "    for i in range(num_docs):\n",
    "        words = [w for w in ConvertSentence2Word(X[i]) if not w in stop]\n",
    "        for w in words:\n",
    "            bagofwords[w] += 1\n",
    "\n",
    "    bagofwords={k:v for k,v in bagofwords.items() if v >= 3} # Remove items apper less than 3\n",
    "    # Word to Index\n",
    "    word2idx = {w:i+1 for i, w in enumerate(bagofwords)} # Start from 1\n",
    "    num_words = len(word2idx)    \n",
    "\n",
    "    # load in pre-trained word vectors\n",
    "    word2vec = {}\n",
    "    start = time.time()\n",
    "    with open(os.path.join(datapath2+'glove.6B.%sd.txt' % EMBEDDING_DIM),encoding='UTF8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM)) # Index 0 will be zero\n",
    "    for word, i in word2idx.items():\n",
    "        if word in word2vec:\n",
    "            embedding_matrix[i] = word2vec[word]\n",
    "    \n",
    "    return word2idx, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word2idx, embedding_matrix = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# save the model\n",
    "with open(datapath1 + 'word2idx_amazon_100k_pet.json', 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "    \n",
    "np.savez(datapath1 + 'WordEmbedding_amazon_100k_pet.npz', embedding_matrix )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
